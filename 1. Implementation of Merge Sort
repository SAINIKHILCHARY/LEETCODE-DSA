Merge Sort has a time complexity of O(n log n) in both the average and worst cases. This means that the time it takes to sort a list of size n grows proportionally to n log n. Here's a breakdown of why:

Merge sort follows a divide-and-conquer approach [3]. It divides the input list into smaller sublists recursively until each sublist has only one element.

Dividing the list can be done in linear time (O(n)) [4].

Merging the sorted sublists back together also takes linear time in the worst case, where each element needs to be compared at most once [4].

The number of times the list is divided is logarithmic (log n) in the size of the list [2]. This is because the list is halved at each division step.

Since both dividing and merging take linear time, and the division happens a logarithmic number of times, the overall time complexity becomes O(n log n).

In conclusion, the efficiency of merge sort comes from its clever division and merging strategies, resulting in a time complexity that grows proportionally to the logarithm of the input size, making it a very efficient sorting algorithm for large datasets.